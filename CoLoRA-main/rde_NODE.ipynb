{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import jax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_mu train: 10, n_mu test: 2, n_variables: 2, n_time samples: 101, n_x samples: 513\n"
     ]
    }
   ],
   "source": [
    "from colora.data import load_all_hdf5, split_data_by_mu, prepare_coordinate_data\n",
    "problem = 'rde'\n",
    "data_dir = Path('./data')\n",
    "data_path = data_dir / problem\n",
    "mus, sols, spacing = load_all_hdf5(data_path)\n",
    "\n",
    "train_mus = np.asarray([2.0, 2.1, 2.2, 2.4, 2.5, 2.6, 2.8, 2.9, 3.0,3.1])\n",
    "test_mus = np.asarray([2.3, 2.7])\n",
    "train_sols, test_sols = split_data_by_mu(mus, sols, train_mus, test_mus) # mus X variables X time X space_x X space_y\n",
    "n_mu_train, n_q, n_t, n_x1 = train_sols.shape\n",
    "n_mu_test, n_q, n_t, n_x1 = test_sols.shape\n",
    "time = spacing[1]\n",
    "x_space = spacing[2]\n",
    "\n",
    "print(f'n_mu train: {n_mu_train}, n_mu test: {n_mu_test}, n_variables: {n_q}, n_time samples: {n_t}, n_x samples: {n_x1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, mu_t_train, X_grid =  prepare_coordinate_data(spacing, train_mus, train_sols)\n",
    "y_test, mu_t_test, X_grid =  prepare_coordinate_data(spacing, test_mus, test_sols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean, std):\n",
    "    return (x-mean)/std\n",
    "\n",
    "mean, std = jnp.mean(mu_t_train, axis=0), jnp.std(mu_t_train, axis=0)\n",
    "mu_t_train = normalize(mu_t_train, mean, std)\n",
    "mu_t_test = normalize(mu_t_test, mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.build import build_colora\n",
    "\n",
    "key = jax.random.PRNGKey(1)\n",
    "\n",
    "domain_len = x_space[-1] - x_space[0]\n",
    "\n",
    "x_dim = 1 # 1 spatial dim\n",
    "mu_t_dim = 2\n",
    "u_dim = 2 # we have two variables here\n",
    "\n",
    "u_layers = ['P', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] # seven colora layers with 1 alpha each means we will have laten dim of 7\n",
    "h_layers = ['D', 'D', 'D']\n",
    "rank = 3\n",
    "full = True # here we allow 3 alphas per colora layer, resulting in a larger latent dimension \n",
    "\n",
    "u_hat_config = {'width': 25, 'layers': u_layers}\n",
    "h_config = {'width': 15, 'layers': h_layers}\n",
    "\n",
    "u_hat_fn, h_fn, theta_init, psi_init = build_colora(\n",
    "    u_hat_config, h_config, x_dim, mu_t_dim, u_dim, lora_filter=['alpha'], period=[domain_len], rank=rank, full=full, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_v_mu_t = vmap(h_fn, in_axes=(None, 0)) # vmap over mu_t array to generate array of phis\n",
    "u_hat_v_x =  vmap(u_hat_fn, in_axes=(None, None, 0)) # vmaped over x to generate solution field over space points\n",
    "u_hat_v_x_phi =  vmap(u_hat_v_x, in_axes=(None, 0, None)) # vmaped over x to generate solution field over space points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(psi_theta, mu_t, X_grid):\n",
    "    psi, theta = psi_theta\n",
    "    phis = h_v_mu_t(psi, mu_t)\n",
    "    pred = u_hat_v_x_phi(theta, phis, X_grid)\n",
    "    return pred\n",
    "\n",
    "def relative_loss_fn(psi_theta, mu_t, sols, X_grid):\n",
    "    pred = predict(psi_theta, mu_t, X_grid)\n",
    "    loss = jnp.linalg.norm(\n",
    "        sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "    return loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.data import Dataset\n",
    "\n",
    "# the dataset is just responsible for batching the data over the mu_t tensor \n",
    "# in order to aviod memory overflow on the GPU\n",
    "dataset = Dataset(mu_t_train, X_grid, y_train, n_batches=15, key=key)\n",
    "dataset = iter(dataset)\n",
    "def args_fn():\n",
    "    return next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 100/100 [00:05<00:00, 17.90it/s, loss=8.305E-02]\n"
     ]
    }
   ],
   "source": [
    "from colora.adam import adam_opt\n",
    "\n",
    "psi_theta = (psi_init, theta_init)\n",
    "opt_psi_theta, loss_history = adam_opt(psi_theta, relative_loss_fn, args_fn, steps=100, learning_rate=5e-3, verbose=True)\n",
    "opt_psi, opt_theta = opt_psi_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 513, 2)\n",
      "(2, 2, 101, 513)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = predict(opt_psi_theta, mu_t_test, X_grid)\n",
    "print(pred.shape)\n",
    "pred = rearrange(pred, '(M T) (N1) Q -> M Q T N1', Q=n_q, T=n_t, N1=n_x1)\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 103626) (2, 103626)\n",
      "Test mean relative error: 8.88E-02\n"
     ]
    }
   ],
   "source": [
    "test_vec =  rearrange(test_sols, 'M Q T N1 -> M (Q T N1)')\n",
    "pred_vec =  rearrange(pred, 'M Q T N1 -> M (Q T N1)')\n",
    "print(test_vec.shape, pred_vec.shape)\n",
    "\n",
    "rel_err = np.linalg.norm(test_vec- pred_vec, axis=1)/np.linalg.norm(test_vec, axis=1)\n",
    "mean_rel_err = rel_err.mean()\n",
    "print(f'Test mean relative error: {mean_rel_err:.2E}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.plot import line_movie\n",
    "\n",
    "line_movie([pred[0][0], test_sols[0][0]], save_to='./img/rde.gif', t=spacing[1], title='RDE', frames=100, x=x_space, legend=['CoLoRA', 'True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phis = h_v_mu_t(opt_psi, mu_t_test)\n",
    "phis = rearrange(phis, '(M T) D -> M T D', T=n_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.plot import trajectory_movie\n",
    "\n",
    "trajectory_movie(phis[0], x=time, title='RDE', ylabel=r'$\\phi(t;\\mu)$', save_to='./img/rde_dynamics', frames=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NeuralODE part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial condition phi(0, mu) from the trained hypernetwork\n",
    "def get_all_phi_0(psi, mu):\n",
    "    mu_0 = jnp.column_stack((mu, jnp.zeros(mu.shape[0])))\n",
    "    # normalize mu_0\n",
    "    mu_0 = normalize(mu_0, mean, std)\n",
    "    return h_v_mu_t(psi, mu_0)\n",
    "\n",
    "\n",
    "# calculate phi0 for train mus\n",
    "phi_0_mus_train = get_all_phi_0(opt_psi, train_mus)\n",
    "\n",
    "print(phi_0_mus_train.shape)\n",
    "\n",
    "'''Proposed approach: \\partial_t phi(t,mu ) = g(phi(t,mu), mu ;omega)\n",
    "CHANGE ACTIVATION OF MLP ?\n",
    "'''\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import diffrax\n",
    "\n",
    "# Define the ODE function approximator using an MLP\n",
    "class ODEFunc(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, depth, key):\n",
    "        super().__init__()\n",
    "        self.mlp = eqx.nn.MLP(input_dim, output_dim, hidden_dim, depth,activation=jax.nn.relu, key=key)\n",
    "\n",
    "    def __call__(self, t, phi_mu):\n",
    "        # Make sure mu is correctly broadcasted\n",
    "        #mu_expanded = jnp.broadcast_to(mu, (phi.shape[0], 1))  # Ensure mu is repeated for each batch of phi\n",
    "        inputs = phi_mu\n",
    "        outputs=self.mlp(inputs)\n",
    "        value_outputs=jax.device_get(outputs)\n",
    "        print(\"MLP output at time\", t, \":\", value_outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Define the Neural ODE class\n",
    "\n",
    "class NeuralODE(eqx.Module):\n",
    "    ode_func: ODEFunc\n",
    "\n",
    "    def __init__(self, phi_dim, mu_dim, hidden_dim, depth, key):\n",
    "        super().__init__()\n",
    "        self.ode_func = ODEFunc(phi_dim+mu_dim, hidden_dim, phi_dim, depth, key)  # output_dim matches phi_dim\n",
    "\n",
    "    def __call__(self, phi0, mu, t_span):\n",
    "        def func(t, phi, args):\n",
    "            #concatenate phi and mu\n",
    "            phi_mu=jnp.concatenate((jnp.array(phi), jnp.array([mu])), axis=0)\n",
    "            return self.ode_func(t, phi_mu)\n",
    "\n",
    "        solver = diffrax.Tsit5()\n",
    "        saveat = diffrax.SaveAt(ts=t_span)\n",
    "        sol = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(func),\n",
    "            solver,\n",
    "            t0=t_span[0],\n",
    "            t1=t_span[-1],\n",
    "            dt0=t_span[1] - t_span[0],\n",
    "            y0=phi0,\n",
    "            saveat=saveat\n",
    "        )\n",
    "        return sol.ys\n",
    "\n",
    "# Quick example to check the shape\n",
    "key = PRNGKey(13131)\n",
    "phi_dim = 7\n",
    "mu_dim = 1   \n",
    "hidden_dim = 10\n",
    "depth = 3\n",
    "\n",
    "# build g\n",
    "g = NeuralODE(phi_dim, mu_dim, hidden_dim, depth, key) # g takes in phi and mu and outputs the time derivative of phi\n",
    "\n",
    "#record tree shape\n",
    "omega,omega_def = jax.tree_util.tree_flatten(g)\n",
    "\n",
    "print(omega)\n",
    "# Prediction and Loss\n",
    "\n",
    "#@eqx.filter_grad\n",
    "# import functools as ft\n",
    "# @ft.partial(jax.jit,static_argnums=1)\n",
    "\n",
    "def predictNODE(omega_flat_theta, g_minus_omega, omega_def, t_span, mus, X_grid, phi_0):\n",
    "    omega_flat, theta = omega_flat_theta\n",
    "    omega_tree = jax.tree_util.tree_unflatten(omega_def, omega_flat)\n",
    "    g=eqx.combine(g_minus_omega,omega_tree)\n",
    "    g_forall_phi_mu = vmap(g, in_axes=(0, 0, None)) # pack (phi0_i,mu_i)\n",
    "\n",
    "    # get phis\n",
    "    phis=g_forall_phi_mu(phi_0, mus, t_span)\n",
    "    # reshape phis to match the shape of sols later\n",
    "    phis=phis.reshape(-1,phi_dim)\n",
    "\n",
    "\n",
    "    pred = u_hat_v_x_phi(theta, phis, X_grid)\n",
    "    return pred\n",
    "\n",
    "def lossNODE(omega_flat_theta, g_minus_omega, omega_def, t_span, mus, sols, X_grid, phi_0):\n",
    "    #omega_flat, theta = omega_flat_theta\n",
    "    pred = predictNODE(omega_flat_theta, g_minus_omega, omega_def, t_span, mus, X_grid,phi_0)\n",
    "\n",
    "    loss = jnp.linalg.norm(\n",
    "        sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "    return loss.mean()\n",
    "\n",
    "t_span = jnp.linspace(0.0, 1.0, 51)\n",
    "\n",
    "omega_flat_theta = (omega_flat, opt_theta)\n",
    "\n",
    "ys = y_train\n",
    "mus = train_mus\n",
    "#g_forall_phi_mu(phi_0_mus_train, train_mus, t_span)\n",
    "loss=lossNODE(omega_flat_theta, g_minus_omega, omega_def, t_span, mus[0:1], ys[0:1], X_grid, phi_0_mus_train[0:1])\n",
    "print(loss)\n",
    "eqxgrad_lossNODE = eqx.filter_value_and_grad(lossNODE)\n",
    "\n",
    "#not flattened pred and loss (see equinox faq)\n",
    "\n",
    "#@eqx.filter_grad\n",
    "# import functools as ft\n",
    "# @ft.partial(jax.jit,static_argnums=1)\n",
    "\n",
    "def predictNODE(omega_theta, omega_def, t_span, mus, X_grid, phi_0):\n",
    "    omega, theta = omega_theta\n",
    "    g=jax.tree_util.tree_unflatten(omega_def, omega)\n",
    "    g_forall_phi_mu = vmap(g, in_axes=(0, 0, None)) # pack (phi0_i,mu_i)\n",
    "\n",
    "    # get phis\n",
    "    phis=g_forall_phi_mu(phi_0, mus, t_span)\n",
    "    # reshape phis to match the shape of sols later\n",
    "    phis=phis.reshape(-1,phi_dim)\n",
    "\n",
    "    pred = u_hat_v_x_phi(theta, phis, X_grid)\n",
    "    return pred\n",
    "\n",
    "def lossNODE(omega_theta, omega_def, t_span, mus, sols, X_grid, phi_0):\n",
    "    #omega_flat, theta = omega_flat_theta\n",
    "    pred = predictNODE(omega_theta, omega_def, t_span, mus, X_grid,phi_0)\n",
    "\n",
    "    loss = jnp.linalg.norm(\n",
    "        sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "    return loss.mean()\n",
    "\n",
    "t_span = jnp.linspace(0.0, 1.0, 51)\n",
    "\n",
    "omega_theta = (omega, opt_theta)\n",
    "\n",
    "#g_forall_phi_mu(phi_0_mus_train, train_mus, t_span)\n",
    "loss=lossNODE(omega_theta, omega_def, t_span, train_mus, y_train, X_grid, phi_0_mus_train)\n",
    "print(loss)\n",
    "eqxgrad_lossNODE = eqx.filter_value_and_grad(lossNODE)\n",
    "\n",
    "#test: compare eqxgrad with jax grad\n",
    "eqxgrad_lossNODE = eqx.filter_value_and_grad(lossNODE)\n",
    "#grad_lossNODE=jax.value_and_grad(lossNODE)\n",
    "\n",
    "import time as timer\n",
    "\n",
    "#compare the calculation speed\n",
    "t_span=jnp.linspace(0.0,1.0,51)\n",
    "\n",
    "start=timer.time()\n",
    "\n",
    "#compare them on the whole trainign set\n",
    "eqx_loss,eqx_grad=eqxgrad_lossNODE(omega_flat_theta, omega_def, t_span, mus, ys, X_grid, phi_0_mus_train)\n",
    "end=timer.time()\n",
    "'''problematic here'''\n",
    "#full_loss,full_grad=grad_lossNODE(omega_flat_theta, omega_def, t_span, mus, ys, X_grid, phi_0_mus_train)\n",
    "#end1=timer.time()\n",
    "\n",
    "\n",
    "print(full_loss)\n",
    "print(full_eqx_loss)\n",
    "print(end-start)\n",
    "print(end1-end)\n",
    "#training \n",
    "ys = y_train\n",
    "mus = train_mus\n",
    "import optax\n",
    "optimizer=optax.adam(1e-3)\n",
    "opt_state=optimizer.init(omega_flat_theta)\n",
    "#@eqx.filter_jit\n",
    "def update(omega_flat_theta, g_minus_omega, omega_def, t_span, mus, sols, X_grid, phi_0, opt_state):\n",
    "    loss,grad = eqxgrad_lossNODE(omega_flat_theta, g_minus_omega, omega_def, t_span, mus, sols, X_grid, phi_0)\n",
    "\n",
    "    updates, new_opt_state=optimizer.update(grad,opt_state)\n",
    "    \n",
    "    new_omega_flat_theta=optax.apply_updates(omega_flat_theta,updates)\n",
    "\n",
    "    return new_omega_flat_theta, new_opt_state, loss\n",
    "\n",
    "n_steps=2\n",
    "losses=[]\n",
    "import time as timer\n",
    "for i in range(n_steps):\n",
    "    start=timer.time()\n",
    "    omega_flat_theta, opt_state, loss=update(omega_flat_theta, g_minus_omega, omega_def, t_span, mus, ys, X_grid, phi_0_mus_train, opt_state)\n",
    "    losses.append(loss)\n",
    "    end=timer.time()\n",
    "    print(f\"{i}th iter, loss={loss}, time={end-start}\")\n",
    "print(losses)\n",
    "\n",
    "\n",
    "# not flattened training\n",
    "#training \n",
    "ys = y_train\n",
    "mus = train_mus\n",
    "import optax\n",
    "optimizer=optax.adam(1e-3)\n",
    "opt_state=optimizer.init(eqx.filter(omega_theta, eqx.is_inexact_array))\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update(omega_theta, omega_def, t_span, mus, sols, X_grid, phi_0, opt_state):\n",
    "    loss,grad = eqxgrad_lossNODE(omega_theta, omega_def, t_span, mus, sols, X_grid, phi_0)\n",
    "\n",
    "    updates, new_opt_state=optimizer.update(grad,opt_state)\n",
    "    \n",
    "    new_omega_theta=eqx.apply_updates(omega_theta,updates)\n",
    "\n",
    "    return new_omega_theta, new_opt_state, loss\n",
    "\n",
    "n_steps=1\n",
    "losses=[]\n",
    "import time as timer\n",
    "\n",
    "for i in range(n_steps):\n",
    "    start=timer.time()\n",
    "    omega_theta, opt_state, loss = update(omega_theta, omega_def, t_span, mus, ys, X_grid, phi_0_mus_train, opt_state)\n",
    "    losses.append(loss)\n",
    "    end=timer.time()\n",
    "    print(f\"{i}th iter, loss={loss}, time={end-start}\")\n",
    "print(losses)\n",
    "\n",
    "\n",
    "#now we have the updated omega_flat_theta, we first test its performance on the test set\n",
    "ys = y_test\n",
    "mus = test_mus\n",
    "# note omega_flat_theta is trained\n",
    "phi_0_mus_test = get_all_phi_0(opt_psi, test_mus)\n",
    "loss_on_test=lossNODE(\n",
    "    omega_theta, omega_def, t_span, mus, ys, X_grid, phi_0_mus_test)\n",
    "\n",
    "print(loss_on_test)\n",
    "\n",
    "# we check the same relative loss\n",
    "pred = predictNODE(omega_theta, omega_def, t_span, test_mus, X_grid, phi_0_mus_test)#removed an argument for nonflattend \n",
    "pred = rearrange(pred, '(M T) (N1 N2) Q -> M Q T N1 N2', Q=n_q, T=n_t, N1=n_x1, N2=n_x2)\n",
    "\n",
    "test_vec =  rearrange(test_sols, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "pred_vec =  rearrange(pred, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "rel_err = np.linalg.norm(test_vec- pred_vec, axis=1)/np.linalg.norm(test_vec, axis=1)\n",
    "mean_rel_err = rel_err.mean()\n",
    "print(f'Test mean relative error: {mean_rel_err:.2E}')\n",
    "**Plotting**\n",
    "#now we can plot the prediction given by the trained model\n",
    "from colora.plot import imshow_movie\n",
    "\n",
    "imshow_movie(pred[0][0], save_to='./img/burgers.gif', t=jnp.linspace(0,1,51), title='Burgers', tight=True, live_cbar=True, frames=85)\n",
    "print(phi_0_mus_test.shape)\n",
    "# and the dynamics of phi_i's by integrating g with the trained omega\n",
    "opt_omega,reopt_theta = omega_theta\n",
    "g=jax.tree_util.tree_unflatten(omega_def,opt_omega)\n",
    "g_forall_phi_mu = vmap(g, in_axes=(0, 0, None))\n",
    "\n",
    "phis = g(phi_0_mus_test[0], test_mus[0], t_span)\n",
    "print(phis.shape)\n",
    "\n",
    "\n",
    "from colora.plot import trajectory_movie\n",
    "leg= []\n",
    "for i in range(phis.shape[-1]):\n",
    "    lstr =rf'$\\phi_{i}$'\n",
    "    leg.append(lstr)\n",
    "trajectory_movie(phis, x=t_span, title='Burgers', ylabel=r'$\\phi(t;\\mu)$', legend=leg, save_to='./img/burgers_dynamics', frames=85)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
