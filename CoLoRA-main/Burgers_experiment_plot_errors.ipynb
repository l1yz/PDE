{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments**\n",
    "\n",
    "Alter latent dimension & augmneted dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import jax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.data import load_all_hdf5, split_data_by_mu, prepare_coordinate_data\n",
    "data_dir = Path('./data')\n",
    "data_path = data_dir / 'burgers'\n",
    "mus, sols, spacing = load_all_hdf5(data_path)\n",
    "\n",
    "\n",
    "train_mus = np.asarray([0.001, 0.00199, 0.00298, 0.00496, 0.00595, 0.00694, 0.00892, 0.01])\n",
    "test_mus = np.asarray([0.00397, 0.00793])\n",
    "train_sols, test_sols = split_data_by_mu(mus, sols, train_mus, test_mus) # mus X variables X time X space_x X space_y\n",
    "\n",
    "\n",
    "train_sols_first,train_sols_second = jnp.array_split(train_sols, 2, axis=2)\n",
    "test_sols_first,test_sols_second = jnp.array_split(test_sols, 2, axis=2)\n",
    "\n",
    "n_mu_train, n_q, n_t, n_x1, n_x2 = train_sols.shape\n",
    "n_mu_test, n_q, n_t, n_x1, n_x2 = test_sols.shape\n",
    "time = spacing[1]\n",
    "x_space = spacing[2]\n",
    "y_space = spacing[3]\n",
    "\n",
    "print(f'n_mu train: {n_mu_train}, n_mu test: {n_mu_test}, n_variables: {n_q}, n_time samples: {n_t}, n_x samples: {n_x1}, n_x2 samples: {n_x2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=jnp.linspace(0, 1.0,51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, mu_t_train, X_grid =  prepare_coordinate_data(spacing, train_mus, train_sols)\n",
    "y_test, mu_t_test, X_grid =  prepare_coordinate_data(spacing, test_mus, test_sols)\n",
    "# we check the shapes of the data\n",
    "print(f'y_train: {y_train.shape}, mu_t_train: {mu_t_train.shape}, X_grid: {X_grid.shape}')\n",
    "print(f'y_test: {y_test.shape}, mu_t_test: {mu_t_test.shape}, X_grid: {X_grid.shape}')\n",
    "\n",
    "# we cut everything in half with time\n",
    "\n",
    "y_train_matrix = y_train.reshape(n_mu_train, n_t, n_x1 *n_x2,1)\n",
    "y_test_matrix = y_test.reshape(n_mu_test, n_t, n_x1 *n_x2,1)\n",
    "\n",
    "y_train_first,y_train_second = jnp.array_split(y_train_matrix,2,axis=1)\n",
    "y_test_first,y_test_second = jnp.array_split(y_test_matrix,2,axis=1)\n",
    "\n",
    "# flatten the first two dimensions\n",
    "y_train_first = rearrange(y_train_first, 'mu t x q -> (mu t) x q')\n",
    "y_train_second = rearrange(y_train_second, 'mu t x q -> (mu t) x q')\n",
    "\n",
    "y_test_first = rearrange(y_test_first, 'mu t x q -> (mu t) x q')\n",
    "y_test_second = rearrange(y_test_second, 'mu t x q -> (mu t) x q')\n",
    "\n",
    "t_span_first,t_span_second = jnp.array_split(time,2,axis=0)\n",
    "n_t_first=len(t_span_first)\n",
    "n_t_second=len(t_span_second)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean, std):\n",
    "    return (x-mean)/std\n",
    "\n",
    "mean, std = jnp.mean(mu_t_train, axis=0), jnp.std(mu_t_train, axis=0)\n",
    "mu_t_train = normalize(mu_t_train, mean, std)\n",
    "mu_t_test = normalize(mu_t_test, mean, std)\n",
    "\n",
    "# we reshape mu_t into a matrix of size n_mu_train by n_t\n",
    "mu_t_train_matrix = mu_t_train.reshape(n_mu_train, n_t,2)\n",
    "mu_t_test_matrix = mu_t_test.reshape(n_mu_test, n_t,2)\n",
    "\n",
    "mu_t_train_first,mu_t_train_second = jnp.array_split(mu_t_train_matrix,2,axis=1)\n",
    "mu_t_train_first = rearrange(mu_t_train_first, 'mu t x -> (mu t) x')\n",
    "mu_t_train_second = rearrange(mu_t_train_second, 'mu t x -> (mu t) x')\n",
    "\n",
    "mu_t_test_first,mu_t_test_second = jnp.array_split(mu_t_test_matrix,2,axis=1)\n",
    "mu_t_test_first = rearrange(mu_t_test_first, 'mu t x -> (mu t) x')\n",
    "mu_t_test_second = rearrange(mu_t_test_second, 'mu t x -> (mu t) x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.build import build_colora\n",
    "\n",
    "key = jax.random.PRNGKey(1)\n",
    "\n",
    "\n",
    "d_len_x = x_space[-1] - x_space[0]\n",
    "d_len_y = y_space[-1] - y_space[0]\n",
    "\n",
    "x_dim = 2\n",
    "mu_t_dim = 2\n",
    "u_dim = 1\n",
    "\n",
    "u_layers = ['P', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] # 7 colora layers with 1 alpha each means we will have laten dim of 2\n",
    "h_layers = ['D', 'D', 'D']\n",
    "rank = 3\n",
    "\n",
    "u_hat_config = {'width': 25, 'layers': u_layers}\n",
    "h_config = {'width': 15, 'layers': h_layers}\n",
    "\n",
    "u_hat_fn, h_fn, theta_init, psi_init = build_colora(\n",
    "    u_hat_config, h_config, x_dim, mu_t_dim, u_dim, lora_filter=['alpha'], period=[d_len_x, d_len_y], rank=rank, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_v_mu_t = vmap(h_fn, in_axes=(None, 0)) # vmap over mu_t array to generate array of phis\n",
    "u_hat_v_x =  vmap(u_hat_fn, in_axes=(None, None, 0)) # vmaped over x to generate solution field over space points\n",
    "u_hat_v_x_phi =  vmap(u_hat_v_x, in_axes=(None, 0, None)) # vmaped over x to generate solution field over space points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(psi_theta, mu_t, X_grid):\n",
    "    psi, theta = psi_theta\n",
    "    phis = h_v_mu_t(psi, mu_t)\n",
    "    pred = u_hat_v_x_phi(theta, phis, X_grid)\n",
    "    return pred\n",
    "\n",
    "def relative_loss_fn(psi_theta, mu_t, sols, X_grid):\n",
    "    pred = predict(psi_theta, mu_t, X_grid)\n",
    "    loss = jnp.linalg.norm(\n",
    "        sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.data import Dataset\n",
    "\n",
    "# the dataset is just responsible for batching the data over the mu_t tensor \n",
    "# in order to aviod memory overflow on the GPU\n",
    "dataset = Dataset(mu_t_train, X_grid, y_train, n_batches=15, key=key)\n",
    "dataset = iter(dataset)\n",
    "def args_fn():\n",
    "    return next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.adam import adam_opt\n",
    "\n",
    "psi_theta = (psi_init, theta_init)\n",
    "opt_psi_theta, loss_history = adam_opt(psi_theta, relative_loss_fn, args_fn, steps=100, learning_rate=5e-3, verbose=True)\n",
    "opt_psi, opt_theta = opt_psi_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we first use pickle to save the psi and theta\n",
    "\n",
    "# # trained 100k steps\n",
    "\n",
    "import pickle\n",
    "with open('burgers_opt_psi.pkl', 'wb') as f:\n",
    "    pickle.dump(opt_psi, f)\n",
    "with open('burgers_opt_theta.pkl', 'wb') as f:\n",
    "    pickle.dump(opt_theta, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # load optimal psi and theta\n",
    "import pickle\n",
    "with open('burgers_opt_psi.pkl', 'rb') as f:\n",
    "    opt_psi = pickle.load(f)\n",
    "with open('burgers_opt_theta.pkl', 'rb') as f:\n",
    "    opt_theta = pickle.load(f)\n",
    "\n",
    "opt_psi_theta=(opt_psi,opt_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative loss\n",
    "n_t=len(time)\n",
    "pred = predict(opt_psi_theta, mu_t_test, X_grid)\n",
    "pred = rearrange(pred, '(M T) (N1 N2) Q -> M Q T N1 N2', Q=n_q, T=n_t, N1=n_x1, N2=n_x2)\n",
    "\n",
    "pred_vec =  rearrange(pred, 'M Q T N1 N2 -> M (Q T N1 N2)') \n",
    "pred_vec_for_time = rearrange(pred, 'M Q T N1 N2 -> T (M Q N1 N2)')\n",
    "\n",
    "test_vec =  rearrange(test_sols, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "test_vec_for_time = rearrange(test_sols, 'M Q T N1 N2 -> T (M Q N1 N2)')\n",
    "\n",
    "rel_err = np.linalg.norm(test_vec- pred_vec, axis=1)/np.linalg.norm(test_vec, axis=1)\n",
    "mean_rel_err = rel_err.mean()\n",
    "\n",
    "\n",
    "rel_err_over_time = np.linalg.norm(test_vec_for_time - pred_vec_for_time, axis=1) / np.linalg.norm(test_vec_for_time, axis=1)\n",
    "\n",
    "\n",
    "print(f'Test mean relative error : {mean_rel_err:.2E}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colora.plot import imshow_movie\n",
    "\n",
    "imshow_movie(pred[0][0], save_to='./img/burgers_test/burgers.gif', t=time, title='burgers', tight=True, frames=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phis = h_v_mu_t(opt_psi, mu_t_test)\n",
    "phis = rearrange(phis, '(M T) D -> M T D', T=n_t)\n",
    "phis_all = []\n",
    "phis_all.append(phis[0])\n",
    "\n",
    "from colora.plot import trajectory_movie\n",
    "leg= []\n",
    "for i in range(phis.shape[-1]):\n",
    "    lstr =rf'$\\phi_{i}$'\n",
    "    leg.append(lstr)\n",
    "trajectory_movie(phis[0], x=time, title='burgers', ylabel=r'$\\phi(t;\\mu)$', legend=leg, save_to='./img/burgers_test/burgers_dynamics', ylim =[-5,3] , frames=85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate more data using the model\n",
    "mu_gen_train=jnp.asarray([0.001, 0.0065, 0.012, 0.0175, 0.023, 0.0285, 0.0395, 0.045, 0.0505, 0.056, 0.0615, 0.072, 0.078, 0.0835, 0.089, 0.0945, 0.1])\n",
    "mu_gen_test = jnp.asarray([0.034,0.067])\n",
    "\n",
    "t= jnp.linspace(0, 1.0,51)\n",
    "t_first,t_second=jnp.array_split(t,2,axis=0)\n",
    "\n",
    "mu_t_gen_train = []\n",
    "for i in range(len(mu_gen_train)):\n",
    "    for j in range(len(t)):\n",
    "        mu_t_gen_train.append([mu_gen_train[i],t[j]])\n",
    "mu_t_gen_train = jnp.asarray(mu_t_gen_train)\n",
    "\n",
    "mu_t_gen_train_first = []\n",
    "for i in range(len(mu_gen_train)):\n",
    "    for j in range(len(t_first)):\n",
    "        mu_t_gen_train_first.append([mu_gen_train[i],t[j]])\n",
    "mu_t_gen_train_first = jnp.asarray(mu_t_gen_train)\n",
    "\n",
    "mu_t_gen_train_second = []\n",
    "for i in range(len(mu_gen_train)):\n",
    "    for j in range(len(t_second)):\n",
    "        mu_t_gen_train_second.append([mu_gen_train[i],t[j]])\n",
    "mu_t_gen_train_second = jnp.asarray(mu_t_gen_train)\n",
    "\n",
    "mu_t_gen_test = []\n",
    "for i in range(len(mu_gen_test)):\n",
    "    for j in range(len(t)):\n",
    "        mu_t_gen_test.append([mu_gen_test[i],t[j]])\n",
    "mu_t_gen_test = jnp.asarray(mu_t_gen_test)\n",
    "\n",
    "mu_t_gen_test_first = []\n",
    "for i in range(len(mu_gen_test)):\n",
    "    for j in range(len(t_first)):\n",
    "        mu_t_gen_test_first.append([mu_gen_test[i],t[j]])\n",
    "mu_t_gen_test_first = jnp.asarray(mu_t_gen_test)\n",
    "\n",
    "mu_t_gen_test_second = []\n",
    "for i in range(len(mu_gen_test)):\n",
    "    for j in range(len(t_second)):\n",
    "        mu_t_gen_test_second.append([mu_gen_test[i],t[j]])\n",
    "mu_t_gen_test_second = jnp.asarray(mu_t_gen_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_gen_train = predict(opt_psi_theta,mu_t_gen_train,X_grid)\n",
    "y_gen_train_first= predict(opt_psi_theta,mu_t_gen_train_first,X_grid)\n",
    "\n",
    "y_gen_test = predict(opt_psi_theta,mu_t_gen_test,X_grid)\n",
    "\n",
    "y_gen_test_first = predict(opt_psi_theta,mu_t_gen_test_first,X_grid)\n",
    "\n",
    "y_gen_train += 5e-3*jax.random.normal(jax.random.PRNGKey(123),y_gen_train.shape)\n",
    "y_gen_train_first += 5e-3*jax.random.normal(jax.random.PRNGKey(123),y_gen_train_first.shape)\n",
    "\n",
    "# these data are used to train NODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we now build a new hypernetwork to train only on the first half, and show that it doesn't extrapolate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build h_new\n",
    "_, h_fn_new, _, psi_init_new = build_colora(\n",
    "    u_hat_config, h_config, x_dim, mu_t_dim, u_dim, lora_filter=['alpha'],period=[d_len_x, d_len_y], rank=rank, key=key)\n",
    "print(h_fn_new)\n",
    "h_new_v_mu_t = vmap(h_fn_new, in_axes=(None, 0)) \n",
    "# then we train h_fn_new on the first half of the time samples using the opt_theta\n",
    "def relative_loss_fn_new(psi, mu_t, sols, X_grid):\n",
    "    phis = h_new_v_mu_t(psi, mu_t)\n",
    "    pred = u_hat_v_x_phi(opt_theta, phis, X_grid)\n",
    "    loss = jnp.linalg.norm(\n",
    "        sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process only the first half\n",
    "\n",
    "from colora.data import Dataset\n",
    "dataset_first = Dataset(mu_t_train_first, X_grid, y_train_first, n_batches=15, key=key)\n",
    "\n",
    "dataset_first = iter(dataset_first)\n",
    "def args_fn_new():\n",
    "    return next(dataset_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start training\n",
    "from colora.adam import adam_opt\n",
    "psi_new = psi_init_new\n",
    "opt_psi_new, loss_history_new = adam_opt(psi_new, relative_loss_fn_new, args_fn_new, steps=100, learning_rate=5e-3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store opt_psi_new\n",
    "# import pickle\n",
    "# with open('burgers_opt_psi_new.pkl', 'wb') as f:\n",
    "#     pickle.dump(opt_psi_new, f)\n",
    "#read it\n",
    "import pickle\n",
    "with open('burgers_opt_psi_new.pkl', 'rb') as f:\n",
    "    opt_psi_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction\n",
    "# test relative loss \n",
    "n_t=len(time)\n",
    "opt_psi_new_opt_theta=(opt_psi_new,opt_theta)\n",
    "\n",
    "pred = predict(opt_psi_new_opt_theta, mu_t_test, X_grid)\n",
    "\n",
    "pred = rearrange(pred, '(M T) (N1 N2) Q -> M Q T N1 N2', Q=n_q, T=n_t, N1=n_x1, N2=n_x2)\n",
    "\n",
    "pred_vec =  rearrange(pred, 'M Q T N1 N2 -> M (Q T N1 N2)') \n",
    "pred_vec_for_time = rearrange(pred, 'M Q T N1 N2 -> T (M Q N1 N2)')\n",
    "\n",
    "test_vec =  rearrange(test_sols, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "test_vec_for_time = rearrange(test_sols, 'M Q T N1 N2 -> T (M Q N1 N2)')\n",
    "\n",
    "rel_err = np.linalg.norm(test_vec- pred_vec, axis=1)/np.linalg.norm(test_vec, axis=1)\n",
    "mean_rel_err = rel_err.mean()\n",
    "\n",
    "\n",
    "rel_err_over_time_h_new = np.linalg.norm(test_vec_for_time - pred_vec_for_time, axis=1) / np.linalg.norm(test_vec_for_time, axis=1)\n",
    "\n",
    "\n",
    "print(f'Test mean relative error : {mean_rel_err:.2E}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from colora.plot import imshow_movie\n",
    "\n",
    "imshow_movie(pred[0][0], save_to='./img/burgers_test/burgers_new_h.gif', t=time, title='burgers: new h trained on [0,2.5]', tight=True, live_cbar=True, frames=85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also the trajectories\n",
    "phis = h_v_mu_t(opt_psi_new, mu_t_test)\n",
    "\n",
    "n_t=len(time)\n",
    "\n",
    "phis = rearrange(phis, '(M T) D -> M T D', T=n_t)\n",
    "\n",
    "phis_all.append(phis[0])\n",
    "\n",
    "from colora.plot import trajectory_movie\n",
    "leg= []\n",
    "for i in range(phis.shape[-1]):\n",
    "    lstr =rf'$\\phi_{i}$'\n",
    "    leg.append(lstr)\n",
    "#plot latent trajectories for the 1st test mu.\n",
    "trajectory_movie(phis[0], x=time, title='burgers: new h trained on [0,2.5]', ylabel=r'$\\phi(t;\\mu)$', legend=leg, save_to='./img/burgers_test/burgers_new_h_dynamics', frames=85) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn the dynamics using Neural ODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial condition phi(0, mu) from the trained hypernetwork\n",
    "def get_all_phi_0(psi, mu):\n",
    "    mu_0 = jnp.column_stack((mu, jnp.zeros(mu.shape[0])))\n",
    "    # normalize mu_0\n",
    "    mu_0 = normalize(mu_0, mean, std)\n",
    "    return h_v_mu_t(psi, mu_0)\n",
    "\n",
    "\n",
    "# calculate phi0 for train mus\n",
    "phi_0_mus_train = get_all_phi_0(opt_psi, mu_gen_train)\n",
    "phi_0_mus_test= get_all_phi_0(opt_psi, mu_gen_test)\n",
    "\n",
    "\n",
    "print(phi_0_mus_train.shape)\n",
    "print(phi_0_mus_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Proposed approach: \\partial_t phi(t,mu ) = g(phi(t,mu), mu ;omega)\n",
    "CHANGE ACTIVATION OF MLP ?\n",
    "'''\n",
    "from colora.NODE import NODE\n",
    "\n",
    "\n",
    "# Quick example to check the shape\n",
    "keygen = 123\n",
    "phi_dim = 7\n",
    "'''check this'''\n",
    "mu_dim = 1   \n",
    "hidden_dim = 10\n",
    "depth = 2\n",
    "\n",
    "g = NODE(phi_dim, mu_dim, hidden_dim, depth, keygen,'quadratic') # g takes in phi and mu and outputs the time derivative of phi\n",
    "#record tree shape\n",
    "omega,omega_def = jax.tree_util.tree_flatten(g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOT AUGMENTED definitions: train omega_theta together\n",
    "\n",
    "\n",
    "# import equinox as eqx\n",
    "# def predictNODE(omega_theta, omega_def, t_span, mus, X_grid, phi_0):\n",
    "#     omega, theta = omega_theta\n",
    "#     g=jax.tree_util.tree_unflatten(omega_def, omega)\n",
    "#     g_forall_phi_mu = vmap(g, in_axes=(0, 0, None)) # pack (phi0_i,mu_i)\n",
    "\n",
    "#     # get phis\n",
    "#     phis=g_forall_phi_mu(phi_0, mus, t_span)\n",
    "#     # reshape phis to match the shape of sols later\n",
    "#     phis=phis.reshape(-1,phi_dim)\n",
    "\n",
    "#     pred = u_hat_v_x_phi(theta, phis, X_grid)\n",
    "#     return pred\n",
    "\n",
    "# def lossNODE(omega_theta, omega_def, t_span, mus, sols, X_grid, phi_0):\n",
    "#     #omega_flat, theta = omega_flat_theta\n",
    "#     pred = predictNODE(omega_theta, omega_def, t_span, mus, X_grid,phi_0)\n",
    "\n",
    "#     loss = jnp.linalg.norm(\n",
    "#         sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "#     return loss.mean()\n",
    "\n",
    "# t_span = t_span_first\n",
    "\n",
    "# omega_theta = (omega, opt_theta)\n",
    "\n",
    "# #g_forall_phi_mu(phi_0_mus_train, train_mus, t_span)\n",
    "# loss=lossNODE(omega_theta, omega_def, t_span, train_mus, y_train_first, X_grid, phi_0_mus_train)\n",
    "# print(loss)\n",
    "\n",
    "# eqxgrad_lossNODE = eqx.filter_value_and_grad(lossNODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train omega only: define predict and loss\n",
    "\n",
    "import equinox as eqx\n",
    "def predictNODE_freeze(omega, theta, omega_def, t_span, mus, X_grid, phi_0):\n",
    "    g=jax.tree_util.tree_unflatten(omega_def, omega)\n",
    "    g_forall_phi_mu = vmap(g, in_axes=(0, 0, None)) # pack (phi0_i,mu_i)\n",
    "\n",
    "    # get phis\n",
    "    phis=g_forall_phi_mu(phi_0, mus, t_span)\n",
    "    # reshape phis to match the shape of sols later\n",
    "    phis=phis.reshape(-1,phi_dim)\n",
    "\n",
    "    pred = u_hat_v_x_phi(theta, phis, X_grid)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def regularized_lossNODE_freeze(omega, theta, omega_def, t_span, mus, sols, X_grid, phi_0):\n",
    "    # Original loss\n",
    "    pred = predictNODE_freeze(omega, theta, omega_def, t_span, mus, X_grid, phi_0)\n",
    "\n",
    "    main_loss = jnp.linalg.norm(sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "    \n",
    "    g = jax.tree_util.tree_unflatten(omega_def, omega)\n",
    "    g_for_all_mu = vmap(g, in_axes=(0, None, None))\n",
    "\n",
    "    def odefunc_norm(phi, mu):\n",
    "        return jnp.linalg.norm(g(phi, mu, t_span))\n",
    "    \n",
    "    reg_loss = jax.vmap(odefunc_norm)(phi_0, mus).mean()\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = main_loss.mean() + 1e-3 * reg_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# eqxgrad_lossNODE_freeze = eqx.filter_value_and_grad(lossNODE_freeze)\n",
    "eqxgrad_regularized_lossNODE = eqx.filter_value_and_grad(regularized_lossNODE_freeze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the time for taking regularized loss and normal oss\n",
    "# import time as timer\n",
    "# start=timer.time()\n",
    "# regularized_lossNODE_freeze(omega, opt_theta, omega_def, t_span, train_mus, y_train_first, X_grid, phi_0_mus_train)\n",
    "# end=timer.time()\n",
    "# print(f'Time for regularized loss: {end-start}')\n",
    "# start=timer.time()\n",
    "# lossNODE_freeze(omega, opt_theta, omega_def, t_span, train_mus, y_train_first, X_grid, phi_0_mus_train)\n",
    "# end=timer.time()\n",
    "# print(f'Time for normal loss: {end-start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_span = t_span_first\n",
    "\n",
    "ys = y_train_first\n",
    "mus = train_mus\n",
    "\n",
    "n_steps=2\n",
    "\n",
    "\n",
    "losses=[]\n",
    "learning_rates=[]\n",
    "\n",
    "theta = opt_theta\n",
    "\n",
    "import optax\n",
    "#define cosine decay\n",
    "init_lr = 1e-3\n",
    "decay_steps = n_steps  # Total number of training steps\n",
    "lr_schedule = optax.cosine_decay_schedule(init_lr, 2*decay_steps)\n",
    "optimizer=optax.chain(optax.adam(learning_rate=lr_schedule),\n",
    "                      optax.clip_by_global_norm(1.0))\n",
    "\n",
    "opt_state=optimizer.init(eqx.filter(omega, eqx.is_inexact_array))\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update(omega,theta, omega_def, t_span, mus, sols, X_grid, phi_0, opt_state):\n",
    "    \n",
    "    loss,grad = eqxgrad_regularized_lossNODE(omega,theta, omega_def, t_span, mus, sols, X_grid, phi_0)\n",
    "\n",
    "    updates, new_opt_state=optimizer.update(grad,opt_state)\n",
    "    # we set the update of theta to zero\n",
    "    new_omega=eqx.apply_updates(omega,updates)\n",
    "\n",
    "    return new_omega, new_opt_state, loss\n",
    "\n",
    "\n",
    "import time as timer\n",
    "#check test loss\n",
    "test_losses=[]\n",
    "test_vec =  rearrange(test_sols, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "    start=timer.time()\n",
    "    omega, opt_state, loss = update(omega, opt_theta, omega_def, t_span, mus, ys, X_grid, phi_0_mus_train, opt_state)\n",
    "    losses.append(loss)\n",
    "    lr = lr_schedule(i)\n",
    "    learning_rates.append(lr)\n",
    "    if i % 50 ==0:\n",
    "        end=timer.time()\n",
    "        #check test loss\n",
    "        pred = predictNODE_freeze(omega,opt_theta, omega_def, time, test_mus, X_grid, phi_0_mus_test) \n",
    "        pred = rearrange(pred, '(M T) (N1 N2) Q -> M Q T N1 N2', Q=n_q, T=len(time), N1=n_x1, N2=n_x2)\n",
    "        \n",
    "        pred_vec =  rearrange(pred, 'M Q T N1 N2 -> M (Q T N1 N2)') \n",
    "        rel_err = np.linalg.norm(test_vec- pred_vec, axis=1)/np.linalg.norm(test_vec, axis=1)\n",
    "        mean_rel_err = rel_err.mean()\n",
    "        test_losses.append(mean_rel_err)\n",
    "        print(f\"{i}th iter, train loss={loss}, test loss = {mean_rel_err} lr={lr} time={end-start}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and testing losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label='Training loss')\n",
    "plt.plot(np.arange(0, len(losses), 50), test_losses, label='Testing loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Vlasov NODE losses')\n",
    "plt.legend()\n",
    "\n",
    "# Plot learning rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(learning_rates)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('learning rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vlasov_NODE_losses_and_lr.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can plot the prediction given by the trained model\n",
    "from colora.plot import imshow_movie\n",
    "# we check the same relative loss on the full interval\n",
    "t_span = time\n",
    "n_t = len(t_span)\n",
    "\n",
    "# switch original and augmented NODE here\n",
    "#--------------\n",
    "\n",
    "# modified for training omega only\n",
    "pred = predictNODE_freeze(omega,opt_theta, omega_def, t_span, test_mus, X_grid, phi_0_mus_test)\n",
    "#--------------\n",
    "\n",
    "# # modified for augmented\n",
    "# phi_0_mus_test_aug = jnp.concatenate([phi_0_mus_test,jnp.zeros((n_mu_test,aug_dim))],axis=1)\n",
    "# print(phi_0_mus_test_aug)\n",
    "# pred=predictNODE_aug(omega_theta, omega_def, t_span, test_mus, X_grid, phi_0_mus_aug_test)\n",
    "#--------------\n",
    "\n",
    "print(pred.shape)\n",
    "pred = rearrange(pred, '(M T) (N1 N2) Q -> M Q T N1 N2', Q=n_q, T=n_t, N1=n_x1, N2=n_x2)\n",
    "\n",
    "test_vec =  rearrange(test_sols, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "test_vec_for_time = rearrange(test_sols, 'M Q T N1 N2 -> T (M Q N1 N2)')\n",
    "\n",
    "\n",
    "pred_vec =  rearrange(pred, 'M Q T N1 N2 -> M (Q T N1 N2)') \n",
    "pred_vec_for_time = rearrange(pred, 'M Q T N1 N2 -> T (M Q N1 N2)')\n",
    "rel_err_over_time_NODE = np.linalg.norm(test_vec_for_time - pred_vec_for_time, axis=1) / np.linalg.norm(test_vec_for_time, axis=1)\n",
    "\n",
    "\n",
    "rel_err = np.linalg.norm(test_vec- pred_vec, axis=1)/np.linalg.norm(test_vec, axis=1)\n",
    "mean_rel_err = rel_err.mean()\n",
    "\n",
    "\n",
    "print(f'Test mean relative error : {mean_rel_err:.2E}')\n",
    "\n",
    "# define check_loss function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from colora.plot import imshow_movie\n",
    "\n",
    "imshow_movie(pred[0][0], save_to='./img/burgers_test/burgers_NODE.gif', t=time, title='burgers NODE', tight=True, live_cbar=True, frames=85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the dynamics of phi_i's by integrating g with the trained omega\n",
    "\n",
    "opt_omega = omega\n",
    "\n",
    "phi_0_mus_test = get_all_phi_0(opt_psi, test_mus)\n",
    "\n",
    "g=jax.tree_util.tree_unflatten(omega_def,opt_omega)\n",
    "g_forall_phi_mu = vmap(g, in_axes=(0, 0, None))\n",
    "\n",
    "phis = g_forall_phi_mu(phi_0_mus_test, test_mus, t_span)\n",
    "print(phis.shape)\n",
    "phis_all.append(phis[0])\n",
    "\n",
    "from colora.plot import trajectory_movie\n",
    "leg= []\n",
    "for i in range(phis.shape[-1]):\n",
    "    lstr =rf'$\\phi_{i}$'\n",
    "    leg.append(lstr)\n",
    "trajectory_movie(phis[0], x=t_span, title='burgers NODE', ylabel=r'$\\phi(t;\\mu)$', legend=leg, save_to='./img/burgers_test/burgers_NODE_dynamics', frames=85)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmented NODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Augmented NODE: Definitions '''\n",
    "# we try to add aux dimensions to the phi_0\n",
    "\n",
    "from colora.NODE import NODE\n",
    "keygen = 123\n",
    "phi_dim = 7  #change this if needed \n",
    "mu_dim = 1   \n",
    "hidden_dim = 10\n",
    "\n",
    "aug_dim=7\n",
    "\n",
    "depth = 1\n",
    "\n",
    "\n",
    "phi_0_mus_aug_train=jnp.concatenate([phi_0_mus_train,jnp.zeros((n_mu_train,aug_dim))],axis=1)\n",
    "phi_0_mus_aug_test=jnp.concatenate([phi_0_mus_test,jnp.zeros((n_mu_test,aug_dim))],axis=1)\n",
    "\n",
    "g = NODE(phi_dim+aug_dim, mu_dim, hidden_dim, depth, keygen,'quadratic') # g is a neural ODE\n",
    "\n",
    "omega,omega_def = jax.tree_util.tree_flatten(g)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Augmented NODE: Training 2: train omega  only '''\n",
    "'''We train on the first half of time'''\n",
    "\n",
    "import equinox as eqx\n",
    "\n",
    "\n",
    "def predictNODE_aug(omega,theta, omega_def, t_span, mus, X_grid, phi_0_aug):\n",
    "    g=jax.tree_util.tree_unflatten(omega_def, omega)\n",
    "    g_forall_phi_mu = vmap(g, in_axes=(0, 0, None)) # pack (phi0_i,mu_i)\n",
    "\n",
    "    # get phis\n",
    "    phis=g_forall_phi_mu(phi_0_aug, mus, t_span)\n",
    "    phis=phis[:,:,:phi_dim]\n",
    "    # reshape phis to match the shape of sols later\n",
    "    phis=phis.reshape(-1,phi_dim)\n",
    "\n",
    "    pred = u_hat_v_x_phi(theta, phis, X_grid)\n",
    "    return pred\n",
    "\n",
    "def lossNODE_aug(omega, theta, omega_def, t_span, mus, sols, X_grid, phi_0_aug):\n",
    "    #omega_flat, theta = omega_flat_theta\n",
    "    pred = predictNODE_aug(omega, theta, omega_def, t_span, mus, X_grid,phi_0_aug)\n",
    "\n",
    "    loss = jnp.linalg.norm(\n",
    "        sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "    return loss.mean()\n",
    "\n",
    "eqxgrad_lossNODE = eqx.filter_value_and_grad(lossNODE_aug)\n",
    "\n",
    "\n",
    "#similarly we regularize it\n",
    "def regularized_lossNODE_aug(omega, theta, omega_def, t_span, mus, sols, X_grid, phi_0_aug):\n",
    "    # Original loss\n",
    "    pred = predictNODE_aug(omega, theta, omega_def, t_span, mus, X_grid, phi_0_aug)\n",
    "\n",
    "    main_loss = jnp.linalg.norm(sols - pred, axis=(1,2)) / jnp.linalg.norm(sols, axis=(1,2))\n",
    "    \n",
    "    g = jax.tree_util.tree_unflatten(omega_def, omega)\n",
    "    \n",
    "    def odefunc_norm(phi, mu):\n",
    "        return jnp.linalg.norm(g(phi, mu, t_span))\n",
    "    \n",
    "    reg_loss = jax.vmap(odefunc_norm)(phi_0_aug, mus).mean()\n",
    "\n",
    "    # Combine losses\n",
    "    total_loss = main_loss.mean() + 1e-3 * reg_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# eqxgrad_lossNODE_freeze = eqx.filter_value_and_grad(lossNODE_freeze)\n",
    "eqxgrad_regularized_lossNODE_aug = eqx.filter_value_and_grad(regularized_lossNODE_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_span = t_span_first\n",
    "\n",
    "ys = y_train_first\n",
    "mus = train_mus\n",
    "\n",
    "n_steps=2\n",
    "\n",
    "\n",
    "losses=[]\n",
    "lrs=[]\n",
    "\n",
    "theta = opt_theta\n",
    "\n",
    "import optax\n",
    "#define cosine decay\n",
    "init_lr = 1e-3\n",
    "decay_steps = n_steps  # Total number of training steps\n",
    "lr_schedule = optax.cosine_decay_schedule(init_lr, 2*decay_steps+1)\n",
    "optimizer=optax.adam(learning_rate=lr_schedule)\n",
    "\n",
    "opt_state=optimizer.init(eqx.filter(omega, eqx.is_inexact_array))\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update(omega,theta, omega_def, t_span, mus, sols, X_grid, phi_0, opt_state):\n",
    "    \n",
    "    loss,grad = eqxgrad_regularized_lossNODE_aug(omega,theta, omega_def, t_span, mus, sols, X_grid, phi_0)\n",
    "\n",
    "    updates, new_opt_state=optimizer.update(grad,opt_state)\n",
    "    # we set the update of theta to zero\n",
    "    new_omega=eqx.apply_updates(omega,updates)\n",
    "\n",
    "    return new_omega, new_opt_state, loss\n",
    "\n",
    "\n",
    "import time as timer\n",
    "\n",
    "test_vec= rearrange(test_sols, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "test_losses=[]\n",
    "\n",
    "for i in range(n_steps):\n",
    "    start=timer.time()\n",
    "    omega, opt_state, loss = update(omega, opt_theta, omega_def, t_span, mus, ys, X_grid, phi_0_mus_aug_train, opt_state)\n",
    "    losses.append(loss)\n",
    "    lr = lr_schedule(i)\n",
    "    lrs.append(lr)\n",
    "    if i % 50 ==0:\n",
    "        end=timer.time()\n",
    "        pred=predictNODE_aug(omega,theta, omega_def, time, test_mus, X_grid, phi_0_mus_aug_test)\n",
    "        pred = rearrange(pred, '(M T) (N1 N2) Q -> M Q T N1 N2', Q=n_q, T=len(time), N1=n_x1, N2=n_x2)\n",
    "        # we do the same as in NODE\n",
    "        pred_vec =  rearrange(pred, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "        rel_err = np.linalg.norm(test_vec- pred_vec, axis=1)/np.linalg.norm(test_vec, axis=1)\n",
    "        mean_rel_err = rel_err.mean()\n",
    "        test_losses.append(mean_rel_err)\n",
    "        print(f\"{i}th iter, train loss={loss}, test loss ={mean_rel_err} lr={lr} time={end-start}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and testing losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label='Training loss')\n",
    "plt.plot(np.arange(0, len(losses), 50), test_losses, label='Testing loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.title('burgers NODE augmented losses')\n",
    "plt.legend()\n",
    "\n",
    "# Plot learning rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(learning_rates)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('learning rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('burgers_NODE_aug_losses_and_lr.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we check the same relative loss on the full interval\n",
    "t_span = time\n",
    "n_t = len(t_span)\n",
    "\n",
    "#--------------\n",
    "# modified for augmented\n",
    "phi_0_mus_test_aug = jnp.concatenate([phi_0_mus_test,jnp.zeros((n_mu_test,aug_dim))],axis=1)\n",
    "print(phi_0_mus_test_aug)\n",
    "\n",
    "# if train omega and theta together\n",
    "#pred=predictNODE(omega_theta, omega_def, t_span, test_mus, X_grid, phi_0_mus_test_aug)\n",
    "# if only train mu:\n",
    "pred=predictNODE_aug(omega,theta, omega_def, t_span, test_mus, X_grid, phi_0_mus_aug_test)\n",
    "#--------------\n",
    "\n",
    "print(pred.shape)\n",
    "pred = rearrange(pred, '(M T) (N1 N2) Q -> M Q T N1 N2', Q=n_q, T=n_t, N1=n_x1, N2=n_x2)\n",
    "\n",
    "pred_vec =  rearrange(pred, 'M Q T N1 N2 -> M (Q T N1 N2)') \n",
    "pred_vec_for_time = rearrange(pred, 'M Q T N1 N2 -> T (M Q N1 N2)')\n",
    "\n",
    "test_vec =  rearrange(test_sols, 'M Q T N1 N2 -> M (Q T N1 N2)')\n",
    "test_vec_for_time = rearrange(test_sols, 'M Q T N1 N2 -> T (M Q N1 N2)')\n",
    "\n",
    "rel_err = np.linalg.norm(test_vec- pred_vec, axis=1)/np.linalg.norm(test_vec, axis=1)\n",
    "mean_rel_err = rel_err.mean()\n",
    "\n",
    "\n",
    "rel_err_over_time_NODE_aug = np.linalg.norm(test_vec_for_time - pred_vec_for_time, axis=1) / np.linalg.norm(test_vec_for_time, axis=1)\n",
    "\n",
    "\n",
    "print(f'Test mean relative error : {mean_rel_err:.2E}')\n",
    "\n",
    "from colora.plot import imshow_movie\n",
    "\n",
    "imshow_movie(pred[0][0], save_to='./img/burgers_test/burgers_NODE_aug.gif', t=time, title='burgers NODE augmented', tight=True, live_cbar=True, frames=85)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the dynamics of phi_i's by integrating g with the trained omega\n",
    "\n",
    "# #for trained together \n",
    "# opt_omega,reopt_theta = omega_theta\n",
    "\n",
    "# # for only omega\n",
    "opt_omega = omega\n",
    "phi_0_mus_test = get_all_phi_0(opt_psi, test_mus)\n",
    "phi_0_mus_aug_test = jnp.concatenate([phi_0_mus_test,jnp.zeros((n_mu_test,aug_dim))],axis=1)\n",
    "\n",
    "g=jax.tree_util.tree_unflatten(omega_def,opt_omega)\n",
    "g_forall_phi_mu = vmap(g, in_axes=(0, 0, None))\n",
    "\n",
    "phis = g_forall_phi_mu(phi_0_mus_aug_test, test_mus, t_span)\n",
    "print(phis.shape)\n",
    "phis_all.append(phis[0][:,:phi_dim])\n",
    "\n",
    "from colora.plot import trajectory_movie\n",
    "leg= []\n",
    "for i in range(phis.shape[-1]):\n",
    "    lstr =rf'$\\phi_{i}$'\n",
    "    leg.append(lstr)\n",
    "trajectory_movie(phis[0][:,:phi_dim], x=t_span, title='burgers NODE augmented', ylabel=r'$\\phi(t;\\mu)$', legend=leg, save_to='./img/burgers_test/burgers_NODE_aug_dynamics', frames=85)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot relative errors over time with NODE and augmented NODE together\n",
    "plt.figure()\n",
    "plt.yscale('log')\n",
    "plt.plot(time, rel_err_over_time,label='Colora')\n",
    "plt.plot(time, rel_err_over_time_h_new,label='Colora with new h')\n",
    "plt.plot(time, rel_err_over_time_NODE,label='NODE')\n",
    "plt.plot(time, rel_err_over_time_NODE_aug,label='NODE augmented')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Relative error')\n",
    "plt.title('Relative error over time after training')\n",
    "plt.savefig('burgers_relative_error_over_time.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def multi_experiment_trajectory_snapshot(all_phis, times=None, title='', ylabel='', xlabel='Time', \n",
    "                                         param_names=None, exp_names=None, colors=None, linestyles=None, \n",
    "                                         ylim=None, save_to=None):\n",
    "    \n",
    "    n_experiments = len(all_phis)\n",
    "    n_params = all_phis[0].shape[1]\n",
    "    \n",
    "    if times is None:\n",
    "        times = [np.arange(len(phis)) for phis in all_phis]\n",
    "    elif not isinstance(times, list):\n",
    "        times = [times] * n_experiments\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    gs = GridSpec(1, 2, width_ratios=[3, 1])  # Create a grid with two columns\n",
    "    \n",
    "    ax = fig.add_subplot(gs[0])  # Main plot\n",
    "    leg_ax = fig.add_subplot(gs[1])  # Legend\n",
    "    leg_ax.axis('off')  # Turn off axis for legend\n",
    "    \n",
    "    if ylim is None:\n",
    "        ylim = np.array([min(phis.min() for phis in all_phis), max(phis.max() for phis in all_phis)])\n",
    "    xlim = [min(t.min() for t in times), max(t.max() for t in times)]\n",
    "    \n",
    "    if colors is None:\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, n_params))\n",
    "    if linestyles is None:\n",
    "        linestyles = ['-', '--', ':', '-.'] * (n_experiments // 4 + 1)\n",
    "    if param_names is None:\n",
    "        param_names = [f'$\\\\phi_{{{i}}}$' for i in range(n_params)]\n",
    "    if exp_names is None:\n",
    "        exp_names = [f'Exp {i+1}' for i in range(n_experiments)]\n",
    "    \n",
    "    for i, (phis, t) in enumerate(zip(all_phis, times)):\n",
    "        for j in range(n_params):\n",
    "            ax.plot(t, phis[:, j], color=colors[j], linestyle=linestyles[i])\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Create separate legends for parameters (colors) and experiments (line styles)\n",
    "    param_legend = [plt.Line2D([0], [0], color=colors[i], lw=2) for i in range(n_params)]\n",
    "    exp_legend = [plt.Line2D([0], [0], color='gray', linestyle=linestyles[i], lw=2) for i in range(n_experiments)]\n",
    "    \n",
    "    # Add the legends to the legend axis\n",
    "    leg_ax.legend(param_legend, param_names, loc='upper left', title='Parameters')\n",
    "    leg_ax.legend(exp_legend, exp_names, loc='upper left', bbox_to_anchor=(0, 0.6), title='Experiments')\n",
    "    \n",
    "    # Combine both legends\n",
    "    combined_handles = param_legend + exp_legend\n",
    "    combined_labels = param_names + exp_names\n",
    "    combined_titles = None#['Params'] * n_params + ['Experiments'] * n_experiments\n",
    "    \n",
    "    # Create a new legend with both parameters and experiments\n",
    "    leg = leg_ax.legend(combined_handles, combined_labels, loc='center', title=None)\n",
    "    \n",
    "    # Add subtitle for each group in the legend\n",
    "    # for t, l in zip(combined_titles, leg.get_texts()):\n",
    "    #     l.set_multialignment('left')\n",
    "    #     l.set_text(f'{t}\\n{l.get_text()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_to is not None:\n",
    "        p = Path(save_to).with_suffix('.png')\n",
    "        plt.savefig(p, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot the trajectories of phis on the same plot\n",
    "\n",
    "\n",
    "times = [time, time, time,time]\n",
    "ys=phis_all\n",
    "\n",
    "param_names = [f'$\\\\phi_{i}$' for i in range(7)]\n",
    "exp_names = ['original', 'new hyperNN', 'NODE', 'NODE augmented']\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, 7))  # 7 distinct colors for 7 parameters\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "\n",
    "multi_experiment_trajectory_snapshot(ys, times=times, \n",
    "                                     title='Burgers latent dynamics comparison', \n",
    "                                     ylabel=r'$\\phi(t;\\mu)$', \n",
    "                                     param_names=param_names,\n",
    "                                     exp_names=exp_names,\n",
    "                                     colors=colors,\n",
    "                                     linestyles=linestyles,\n",
    "                                     save_to='./img/vlasov_test/burgers_comparison_dynamics')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
